{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB8vW5yhBF5I",
        "outputId": "63c9a160-330b-46cf-a766-31e3a9992633"
      },
      "outputs": [],
      "source": [
        "# Colab RAG notebook - single cell setup + functions\n",
        "# 1) Install dependencies\n",
        "!pip install -q transformers sentence-transformers faiss-cpu PyPDF2 python-docx accelerate\n",
        "\n",
        "# Packages:\n",
        "# transformers: Hugging Face transformer models (tokenizers, model classes).\n",
        "# sentence-transformers: easy-to-use embedding models (SentenceTransformer wrapper).\n",
        "# faiss-cpu: FAISS library for fast vector similarity search (GPU build). If you use GPU, faiss-gpu.\n",
        "# PyPDF2: extract text from PDFs.\n",
        "# python-docx (imported as docx): read .docx files.\n",
        "# accelerate: helps with model loading and device handling \n",
        "\n",
        "# 2) Imports\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple #type hints to make code clearer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize #normalize vectors easily (for cosine similarity).\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import files\n",
        "\n",
        "# For reading pdf/docx\n",
        "import PyPDF2\n",
        "import docx\n",
        "\n",
        "# 3) Configure model choices (change model names here if you wish)\n",
        "EMBEDDING_MODEL = \"paraphrase-MiniLM-L6-v2\"  # small & fast\n",
        "GEN_MODEL = \"google/flan-t5-small\"           # small generator that runs on T4\n",
        "# Optionally: \"google/flan-t5-base\" for better quality if GPU memory allows\n",
        "\n",
        "# 4) #Add your hugging face token\n",
        "HF_TOKEN = \"\"\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 5) Load models (embedding + generator)\n",
        "print(\"Loading embedding model...\")\n",
        "embedder = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
        "\n",
        "print(\"Loading generator model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, use_auth_token=HF_TOKEN)\n",
        "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL, use_auth_token=HF_TOKEN)\n",
        "gen_model = gen_model.to(device)\n",
        "gen_model.eval() #set model to evaluation mode (disables dropout, etc.)\n",
        "\n",
        "# 6) Utilities: read files and extract text\n",
        "def read_pdf(path: str) -> str:\n",
        "    text_pages = [] \n",
        "    with open(path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for p in reader.pages:\n",
        "            try:\n",
        "                text_pages.append(p.extract_text() or \"\") #attempt to get text\n",
        "            except Exception:\n",
        "                text_pages.append(\"\")\n",
        "    return \"\\n\".join(text_pages)\n",
        "\n",
        "def read_docx(path: str) -> str:\n",
        "    doc = docx.Document(path)\n",
        "    lines = [p.text for p in doc.paragraphs]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def read_txt(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: # text data - converted in machine redable data\n",
        "        return f.read()\n",
        "\n",
        "def load_file_text(path: str) -> str:\n",
        "    path = str(path)\n",
        "    if path.lower().endswith(\".pdf\"):\n",
        "        return read_pdf(path)\n",
        "    if path.lower().endswith(\".docx\"):\n",
        "        return read_docx(path)\n",
        "    if path.lower().endswith(\".txt\"):\n",
        "        return read_txt(path)\n",
        "    raise ValueError(\"Unsupported file type: \" + path)\n",
        "\n",
        "# 7) Chunking function (splits long text into chunks for retrieval)\n",
        "def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    chunk_size: approx tokens/characters per chunk (characters here)\n",
        "    chunk_overlap: overlapping characters between chunks\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\r\", \" \")\n",
        "    chunks = [] \n",
        "    start = 0\n",
        "    length = len(text)\n",
        "    while start < length:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk.strip()) #strip() removes leading/trailing whitespace.\n",
        "        start = end - chunk_overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    # drop empty\n",
        "    chunks = [c for c in chunks if len(c) > 20]\n",
        "    return chunks\n",
        "\n",
        "# 8) Build FAISS index from multiple uploaded files\n",
        "class RAGIndex:\n",
        "    def __init__(self, embedding_dim: int):\n",
        "        self.metadata = []  # meta per vector -> (source, chunk_text)\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def add(self, vectors: np.ndarray, metas: List[Tuple[str,str]]):\n",
        "        vectors = vectors.astype('float32')\n",
        "        if self.embeddings is None:\n",
        "            self.embeddings = vectors #simply assign vectors to self.embeddings\n",
        "        else:\n",
        "            self.embeddings = np.vstack([self.embeddings, vectors]) #stack the new vectors below existing ones using np.vstack\n",
        "        self.metadata.extend(metas) #Extend the metadata list with the new metas\n",
        "\n",
        "    def build_faiss(self):\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"No embeddings added.\")\n",
        "        # L2 index (use inner product after normalization for cosine)\n",
        "        self.index = faiss.IndexFlatIP(self.embedding_dim) #computes Inner Product (IP) between vectors.\n",
        "        # normalize vectors for cosine similarity\n",
        "        normalize(self.embeddings, axis=1, copy=False) #axis=1 means operate across each row (each vector)\n",
        "        self.index.add(self.embeddings) #Why: this is the step that makes the dataset searchable quickly.\n",
        "        print(\"FAISS index built, total vectors:\", self.index.ntotal)\n",
        "\n",
        "    #search takes q_vec (query vector(s)) and returns the top-k closest vectors according to inner product\n",
        "    def search(self, q_vec: np.ndarray, top_k: int = 5):\n",
        "        q = q_vec.astype('float32')\n",
        "        normalize(q, axis=1, copy=False)\n",
        "        # D: distances (here inner product scores) with shape (M, top_k).\n",
        "        # I: indices (ints) of the nearest neighbors with shape (M, top_k).\n",
        "        D, I = self.index.search(q, top_k)\n",
        "        results = [] # [['':'' , '':'', ], ['':'' , '':'', ], ['':'' , '':'', ]]\n",
        "        for dist_list, idx_list in zip(D, I):\n",
        "            batch = [] # ['':'' , '':'', ]\n",
        "            for dist, idx in zip(dist_list, idx_list):\n",
        "                meta = self.metadata[idx]\n",
        "                batch.append({\"score\": float(dist), \"source\": meta[0], \"text\": meta[1]})\n",
        "            results.append(batch)\n",
        "        return results\n",
        "\n",
        "# 9) Full pipeline: upload -> index -> query\n",
        "INDEX = None\n",
        "\n",
        "def upload_and_index_files(paths: List[str], chunk_size: int = 700, chunk_overlap: int = 100):\n",
        "    \"\"\"\n",
        "    paths: list of local file paths (in Colab environment)\n",
        "    \"\"\"\n",
        "    global INDEX\n",
        "    all_texts = []\n",
        "    metas = []\n",
        "    for p in paths:\n",
        "        print(\"Reading:\", p) #gives user feedback\n",
        "        txt = load_file_text(p)\n",
        "        chunks = chunk_text(txt, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        print(f\"  -> {len(chunks)} chunks from {p}\")\n",
        "        for i, c in enumerate(chunks):\n",
        "            metas.append((os.path.basename(p), c)) #os.path.basename(p) extracts the file name (without directories) — this is saved as the source in metadata\n",
        "            all_texts.append(c) #all_texts.append(c) — store the chunk string in all_texts\n",
        "    if len(all_texts) == 0:\n",
        "        raise ValueError(\"No text extracted from files.\")\n",
        "\n",
        "    # compute embeddings in batches\n",
        "    B = 64  #sets an embedding batch size. We will embed 64 chunks at a time\n",
        "    embeddings = []\n",
        "    for i in range(0, len(all_texts), B):\n",
        "        batch = all_texts[i:i+B] #selects a sublist of up to B chunks\n",
        "        emb = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embeddings.append(emb)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    dim = embeddings.shape[1] #stores the embedding dimensionality D. We need this to build the RAGIndex\n",
        "    INDEX = RAGIndex(embedding_dim=dim)\n",
        "    INDEX.add(embeddings, metas)\n",
        "    INDEX.build_faiss()\n",
        "    print(\"Indexing complete.\")\n",
        "\n",
        "def retrieve_contexts(query: str, top_k: int = 5) -> List[dict]:\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True) #Returns a NumPy array q_emb with shape (1, D)\n",
        "    hits = INDEX.search(q_emb, top_k=top_k)[0]\n",
        "    return hits\n",
        "\n",
        "def generate_answer(query: str, top_k: int = 4, max_len: int = 256, temperature: float = 0.1) -> str:\n",
        "    # retrieve\n",
        "    hits = retrieve_contexts(query, top_k=top_k) #Call the retrieval function; hits is a list of top_k dicts\n",
        "    # build prompt: include top chunks with sources\n",
        "    context_texts = []\n",
        "    for h in hits:\n",
        "        # limit chunk length added\n",
        "        chunk = h[\"text\"]\n",
        "        source = h[\"source\"]\n",
        "        context_texts.append(f\"Source: {source}\\n{chunk}\")\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_texts)\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. Use the provided CONTEXT to answer the question. \"\n",
        "        \"If the answer is not contained in the context, say you don't know.\\n\\n\"\n",
        "        f\"CONTEXT:\\n{context}\\n\\n\"\n",
        "        f\"QUESTION: {query}\\n\\nAnswer:\"\n",
        "    )\n",
        "    # tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    #truncates the tokenized input to at most 1024 tokens\n",
        "    with torch.no_grad():\n",
        "        out = gen_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_len,\n",
        "            do_sample=False,\n",
        "            num_beams=4,\n",
        "            early_stopping=True #enables beam search with 4 beams (looks for the best among 4 candidate continuations).\n",
        "        )\n",
        "    answer = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    return answer, hits\n",
        "\n",
        "# 10) Helper: upload files from local machine in Colab\n",
        "def colab_upload_and_index():\n",
        "    print(\"Use the file browser or the button to upload files. You can upload multiple files (pdf, docx, txt).\")\n",
        "    uploaded = files.upload()\n",
        "    saved_paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        saved_paths.append(filename)\n",
        "    upload_and_index_files(saved_paths)\n",
        "    print(\"Uploaded & indexed files:\", saved_paths)\n",
        "\n",
        "# 11) Example interactive loop (run after indexing)\n",
        "def interactive_qa_loop():\n",
        "    print(\"Enter 'exit' to stop.\")\n",
        "    while True:\n",
        "        q = input(\"\\nYour question: \")\n",
        "        if q.strip().lower() in (\"exit\",\"quit\"):\n",
        "            break\n",
        "        ans, hits = generate_answer(q)\n",
        "        display(Markdown(\"**Answer:**\\n\\n\" + ans))\n",
        "        display(Markdown(\"**Top retrieved contexts (debug):**\"))\n",
        "        for i, h in enumerate(hits):\n",
        "            display(Markdown(f\"**{i+1}. Source:** {h['source']}  \\n**Score:** {h['score']:.4f}\\n\\n{h['text'][:700]}...\"))\n",
        "\n",
        "# 12) Example usage instructions printed\n",
        "print(\"\\nREADY.\\nHow to use:\\n1) Run colab_upload_and_index() to upload & index your files.\\n2) Call interactive_qa_loop() to start asking questions.\\n\\nExample:\\n  colab_upload_and_index()\\n  interactive_qa_loop()\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Ob5UyHlfkBzH",
        "outputId": "93f1b63e-7a91-4d32-ff2b-0775630c13c5"
      },
      "outputs": [],
      "source": [
        "# Gradio UI for your existing RAG functions\n",
        "!pip install -q gradio==3.44.0 >/dev/null 2>&1\n",
        "\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Helper to adapt gr.File upload to local paths and call your existing indexer\n",
        "def gradio_index(files, chunk_size=700, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    files: list of temporary file-like objects from Gradio (each has .name)\n",
        "    \"\"\"\n",
        "    if not files:\n",
        "        return \"No files uploaded. Please upload pdf/docx/txt files.\", \"\"\n",
        "    # Save files to current working dir to ensure existing load_file_text can read them\n",
        "    saved_paths = []\n",
        "    for f in files:\n",
        "        src_path = f.name if hasattr(f, \"name\") else f\n",
        "        # ensure a safe destination filename (use original name if present)\n",
        "        dest_name = Path(src_path).name\n",
        "        dest_path = dest_name\n",
        "        # Copy the uploaded temp file to working dir (overwrite if exists)\n",
        "        shutil.copy(src_path, dest_path)\n",
        "        saved_paths.append(dest_path)\n",
        "\n",
        "    try:\n",
        "        upload_and_index_files(saved_paths, chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap))\n",
        "    except Exception as e:\n",
        "        return f\"Indexing failed: {e}\", \"\"\n",
        "    return f\"Indexed {len(saved_paths)} file(s): {', '.join(saved_paths)}\", \", \".join(saved_paths)\n",
        "\n",
        "# Helper to ask question using your generate_answer\n",
        "def gradio_ask(question, top_k=4, max_len=256):\n",
        "    if INDEX is None:\n",
        "        return \"Please upload & index files first.\", \"No retrieved contexts (index empty).\"\n",
        "    try:\n",
        "        ans, hits = generate_answer(question, top_k=int(top_k), max_len=int(max_len))\n",
        "    except Exception as e:\n",
        "        return f\"Error while generating answer: {e}\", \"\"\n",
        "    # format retrieved contexts nicely\n",
        "    sources_md = \"\"\n",
        "    for i, h in enumerate(hits):\n",
        "        snippet = h[\"text\"]\n",
        "        if len(snippet) > 600:\n",
        "            snippet = snippet[:600] + \"...\"\n",
        "        sources_md += f\"**{i+1}. {h['source']}** (score: {h['score']:.4f})\\n\\n{snippet}\\n\\n---\\n\\n\"\n",
        "    return ans, sources_md\n",
        "\n",
        "# Build Gradio interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# RAG — Simple Gradio UI\")\n",
        "    gr.Markdown(\"Upload files (pdf/docx/txt) -> Index -> Ask questions.\\n\\nMake sure you've run the earlier cell that loads the models and defines `upload_and_index_files` and `generate_answer`.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"Upload files (pdf/docx/txt)\", file_count=\"multiple\", type=\"file\")\n",
        "            chunk_size = gr.Slider(200, 2000, value=700, step=50, label=\"Chunk size (characters)\")\n",
        "            chunk_overlap = gr.Slider(0, 500, value=100, step=10, label=\"Chunk overlap (characters)\")\n",
        "            index_btn = gr.Button(\"Upload & Index\")\n",
        "            status = gr.Textbox(label=\"Indexer status\", value=\"No files indexed yet.\", interactive=False)\n",
        "            files_list = gr.Textbox(label=\"Files indexed\", value=\"\", interactive=False)\n",
        "        with gr.Column(scale=2):\n",
        "            qbox = gr.Textbox(label=\"Ask a question about uploaded files\", placeholder=\"Type your question here...\", lines=2)\n",
        "            topk = gr.Slider(1, 8, value=4, step=1, label=\"Top-k retrieved chunks\")\n",
        "            max_len = gr.Slider(64, 512, value=256, step=16, label=\"Max answer tokens\")\n",
        "            ask_btn = gr.Button(\"Ask\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            answer_md = gr.Markdown(\"Answer will appear here.\")\n",
        "        with gr.Column():\n",
        "            contexts_md = gr.Markdown(\"Retrieved contexts / sources will appear here.\")\n",
        "\n",
        "    # Wire up events\n",
        "    index_btn.click(fn=gradio_index, inputs=[file_input, chunk_size, chunk_overlap], outputs=[status, files_list])\n",
        "    ask_btn.click(fn=gradio_ask, inputs=[qbox, topk, max_len], outputs=[answer_md, contexts_md])\n",
        "    qbox.submit(fn=gradio_ask, inputs=[qbox, topk, max_len], outputs=[answer_md, contexts_md])\n",
        "\n",
        "# Launch the app. set share=True if you want a public link.\n",
        "app.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
